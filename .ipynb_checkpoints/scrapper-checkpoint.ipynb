{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "Feature to input into the model :\n",
    "- images with more ponderation than the body of the article ( it can hold more information and impact on the reader )\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from newsapi import NewsApiClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import urllib.request\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- Fetching data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to fetch news data using an original scrapper, including published images, so they be injected into the model as additional features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_one_article(url,_,nbr_articles):\n",
    "    try:\n",
    "        dirName = 'images'\n",
    "        os.mkdir(dirName)\n",
    "    except:\n",
    "        pass\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    csv_file = open('bbc_news.csv', 'a')\n",
    "\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['headline', 'summary', 'image_link', 'category'])\n",
    "    i = 98\n",
    "    try:\n",
    "        title = soup.find_all(class_ = 'gs-c-promo-heading-title')[0].text\n",
    "        Text = article.find(class_='gs-c-promo-summary').text\n",
    "        category = article.find(class_='gs-c-section-link').text.strip()\n",
    "    except Exception as e:\n",
    "        category = 'None'\n",
    "\n",
    "    try:\n",
    "        image_tag = str(article.find('img'))\n",
    "        elements = image_tag.split('src=\"')\n",
    "        image_link = elements[1].split('\"')[0]\n",
    "    except Exception as e:\n",
    "        image_link = None\n",
    "    Content_list = soup.find_all(class_ = 'eq5iqo00')\n",
    "    article_content = build_text(Content_list)\n",
    "    print('fetched the name of author :')\n",
    "    print(\"fetched the article's content : \",article_content[:100],f' +[{len(article_content)-100}] ',)\n",
    "    article_dic= {'id': _ ,'author':None,'article_content':article_content}\n",
    "    \n",
    "    download_images(soup,_,nbr_articles)\n",
    "    return article_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text(list_):\n",
    "    list_.pop()\n",
    "    return '\\n'.join([x.text for x in list_])\n",
    "def dl_img(url, file_path, file_name):\n",
    "    full_path = file_path + file_name + '.jpg'\n",
    "    urllib.request.urlretrieve(url, full_path)\n",
    "    return full_path\n",
    "def download_images(soup_result,article_index,nbr_articles):\n",
    "    nbr_img = 0\n",
    "    try:\n",
    "        os.mkdir(f'images/Article{article_index}')\n",
    "    except:\n",
    "        pass\n",
    "    img_results = [res['src'] for res in soup_result.find_all('img')]\n",
    "    for i in tqdm(range(len(img_results)),desc = f'Downloading images /{nbr_articles}'):\n",
    "        image_link = img_results[i]\n",
    "        image_link_loc = f'images/Article{article_index}/'\n",
    "        file_name = f'img{i}'\n",
    "        try:\n",
    "            image_link_local = dl_img(image_link,image_link_loc, file_name)\n",
    "            i += 1\n",
    "            nbr_img+=1\n",
    "        except Exception as identifier:\n",
    "            image_link_local = \"EXTERNAL: \" + image_link\n",
    "            i += 1\n",
    "    print('\\nArticle',article_index,\"'s\",nbr_img,' images were downloaded succesfully, ',nbr_articles,'articles remains')\n",
    "def get_links_list(soup):\n",
    "        results = soup.find_all('a',class_ = 'gs-c-promo-heading')\n",
    "        links_list = ['http://www.bbc.com/'+res['href'] for res in results]\n",
    "        return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_bbc_articles(category_url):\n",
    "    url = category_url\n",
    "    response = requests.get(url)\n",
    "    print('Request status (200 means a succesful request): ',response.status_code)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    csv_file = open('bbc_news.csv', 'a')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['id', 'title', 'publishing date', 'body'])\n",
    "    links = get_links_list(soup)\n",
    "    print('Fetching data from ',len(links) ,'bbc news climate articles')\n",
    "    results = []\n",
    "    for _ in range(22,len(links)):  # Covers all the articles len(links)+1\n",
    "        print('------------------------------------')\n",
    "        try: \n",
    "            A = scrap_one_article(links[_],_,len(links))\n",
    "            results.append(A)\n",
    "        except: ValueError\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request status (200 means a succesful request):  200\n",
      "Fetching data from  43 bbc news climate articles\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  As the COP26 climate summit enters its second week, negotiations in Glasgow have hit a critical phas  +[5830] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 17/17 [00:05<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 22 's 17  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  It's the end of week one at the COP26 climate conference in Glasgow, and world leaders have already   +[12545] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 18/18 [00:05<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 23 's 18  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  World leaders have pledged to end and reverse deforestation by 2030.\n",
      "But in Brazil's Amazon rainfore  +[4398] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 21/21 [00:05<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 24 's 21  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  There has been criticism of the number of world leaders and other delegates who have travelled to th  +[4785] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 19/19 [00:05<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 25 's 19  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  World temperatures are rising because of human activity, and climate change now threatens every aspe  +[5415] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 26 's 20  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  Scientists and politicians say we are facing a planetary crisis because of climate change. \n",
      "But what  +[4650] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 27 's 17  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "fetched the name of author :\n",
      "fetched the article's content :  Avoiding meat and dairy products is one of the biggest ways to reduce your environmental impact, acc  +[7141] \n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  Countries have set out plans to cut greenhouse gas emissions, at the COP26 climate change summit in   +[4357] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 15/15 [00:04<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 30 's 15  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  Re-flooding coastal wetlands could provide an opportunity to \"work with nature\" and use sea level ri  +[2036] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 14/14 [00:07<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 31 's 14  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  In our monthly feature, Then and Now, we reveal some of the ways that planet Earth has been changing  +[4404] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 14/14 [00:06<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 32 's 14  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  In our monthly feature, Then and Now, we reveal some of the ways that planet Earth has been changing  +[3032] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43: 100%|██████████████████████████████████████████████████████████| 14/14 [00:05<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article 33 's 14  images were downloaded succesfully,  43 articles remains\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:   0%|                                                                   | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched the name of author :\n",
      "fetched the article's content :  In our monthly feature, Then and Now, we reveal some of the ways that planet Earth has been changing  +[5733] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images /43:  29%|█████████████████▎                                         | 5/17 [00:03<00:06,  2.00it/s]"
     ]
    }
   ],
   "source": [
    "climate_url = 'https://www.bbc.com/news/science-environment-56837908'\n",
    "data = scrap_bbc_articles(climate_url)\n",
    "def fetch_clean_data(data_list): \n",
    "    article_results = [] \n",
    "    for i in range(len(data_list)):\n",
    "        try:\n",
    "            article_dict = {}\n",
    "            article_dict['id'] = data_list[i]['id']\n",
    "            article_dict['author'] = data_list[i]['author']\n",
    "            article_dict['article_content'] = data_list[i]['article_content'].strip(\"\\n\").strip(\"\\t\")\n",
    "            article_results.append(article_dict)\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return article_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.DataFrame.from_Dict(fetch_clean_data(data_list))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.to_excel('exported_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> II- Data preprocessing :</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Feature engineering is an important process in the pipeline of any machine learning project.\n",
    "It involves applying a series of transformations to the data, including\n",
    "data, including eliminating bad records, encoding variables, scaling variables and finally eliminating\n",
    "scaling of variables and finally the elimination of variables with a high correlation.\n",
    "The motivation behind this process is to feed the algorithms with digested data\n",
    "and to improve their performance by reducing the learning time.\n",
    "In this project, different data processing techniques were applied to implement the machine learning models.\n",
    "the machine learning models and improve their performance, which we will discuss below.\n",
    "we will discuss below.\n",
    "### Missing value rate and zero variance\n",
    "Here we try to eliminate variables with a very high rate of missing values\n",
    "or zero variance. The following variables are then removed from our final database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "      axis='columns', inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'description', 'url', 'urlToImage', 'publishedAt', 'content'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image source, Getty Images\n",
      "Image caption, Industry leaders argue oil and gas extraction cannot be scaled back too quickly\n",
      "Business leaders have warned that thousands of oil and gas jobs could be at… [+5155 chars]\n"
     ]
    }
   ],
   "source": [
    "print(str(data.iloc[2]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
